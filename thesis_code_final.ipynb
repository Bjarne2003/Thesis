{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4cdd6e",
   "metadata": {},
   "source": [
    "## Random pattern classification with surrogate gradient learning\n",
    "\n",
    "Notebook adapted for Julia from [Friedemann Zenke](https://github.com/fzenke/spytorch/tree/main/notebooks). Tested on Julia v1.7.2. <div style=\"text-align: right\"> &copy; Hartmut Fitz (2024)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71cfee2",
   "metadata": {},
   "source": [
    "### Run between whole experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_accuracies_probabilities=[]\n",
    "diff_smoothed_loss=[]\n",
    "diff_accuracies_probabilities_pen=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a16f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_change=0.6\n",
    "# 0.4\n",
    "# 0.2\n",
    "# 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6349198",
   "metadata": {},
   "source": [
    "### Run between different probabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "using Printf\n",
    "using Flux\n",
    "using Flux.Optimise: update!\n",
    "using Flux.Zygote: pullback, gradient, @adjoint, @ignore, Buffer\n",
    "using Random\n",
    "using Plots\n",
    "using Pkg\n",
    "# Pkg.add(\"Plots\")  # Install if not already done\n",
    "gr(dpi = 300) # set resolution for plots with gr backend\n",
    "loss_values_more =[]\n",
    "accuracies_normal_words = []\n",
    "accuracies_pen=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Interpolations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(235)\n",
    "# 123\n",
    "# 150\n",
    "# 2\n",
    "# 80\n",
    "# 235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f30ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f077418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "cd(\"C:/ru-2023-2024/thesis\")\n",
    "include(\"convert_to_ipa.jl\")\n",
    "using .convert_to_ipa\n",
    "\n",
    "# Example usage\n",
    "words = word_list\n",
    "phon=[]\n",
    "for w in words\n",
    "    p= convert_to_ipa.Convert_to_ipa(w)\n",
    "    append!(phon,p)\n",
    "end\n",
    "for p in phon\n",
    "    print(\"\\\"\",p,\"\\\"\",\",\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d02b3d",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50332637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "    dt = 0.001          # unit is seconds\n",
    "    time = 0.1          # each pattern is \"on\" for 100 ms\n",
    "\n",
    "# Model parameters\n",
    "    tau_mem = 10e-3\n",
    "    beta = exp(-dt/tau_mem)\n",
    "    num_Hidden = 30\n",
    "\n",
    "    letter_Nsteps = 25\n",
    "    num_Inputs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3f62d",
   "metadata": {},
   "source": [
    "### Preprocess input (functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dca264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function encode_sequences(sequences, spike_dict, letter_Nsteps, num_Inputs, max_length)\n",
    "    enc_sequences = []\n",
    "    for s in sequences\n",
    "        sequence_spikes = zeros(Int, letter_Nsteps * max_length, num_Inputs)\n",
    "\n",
    "        for (i, char) in enumerate(s)\n",
    "            spike_pattern = spike_dict[char]\n",
    "            start_index = (i - 1) * letter_Nsteps + 1\n",
    "            end_index = i * letter_Nsteps\n",
    "            sequence_spikes[start_index:end_index, :] .= spike_pattern\n",
    "        end\n",
    "\n",
    "        push!(enc_sequences, sequence_spikes)\n",
    "    end\n",
    "    \n",
    "    return enc_sequences\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function make_spike_alphabet(alphabet, Nsteps, num_Inputs)\n",
    "    spike_alphabet = Vector{Array{Int, 2}}(undef, length(alphabet))  \n",
    "    for (i, letter) in enumerate(alphabet)\n",
    "        spike_letter = ones(Int8, Nsteps, num_Inputs).*rand(Nsteps, num_Inputs).<0.1\n",
    "        spike_alphabet[i] = spike_letter\n",
    "    end\n",
    "    return spike_alphabet\n",
    "end\n",
    "\n",
    "function one_hot_encoding(phoneme_sequences,word_list,mapping)\n",
    "    one_hot_matrix = zeros(Int8,length(phoneme_sequences), length(word_list))\n",
    "    for (s,sequence) in enumerate(phoneme_sequences)\n",
    "        for (w,word) in enumerate(word_list)\n",
    "            if mapping[sequence]==word\n",
    "                one_hot_matrix[s,w]=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return one_hot_matrix\n",
    "end\n",
    "\n",
    "\n",
    "function flip_spikes_with_probability(spike_pattern, probability)\n",
    "    for i in 1:size(spike_pattern, 1)\n",
    "        for j in 1:size(spike_pattern, 2)\n",
    "            if spike_pattern[i, j] == 1\n",
    "                if rand() < probability\n",
    "                    spike_pattern[i, j] = 1 - spike_pattern[i, j]  # Flip 0 to 1 and 1 to 0\n",
    "                end\n",
    "            \n",
    "            else\n",
    "                if rand() < probability/10\n",
    "                    spike_pattern[i, j] = 1 - spike_pattern[i, j]  # Flip 0 to 1 and 1 to 0\n",
    "                end\n",
    "            end\n",
    "        \n",
    "        end\n",
    "    end\n",
    "    return spike_pattern\n",
    "end\n",
    "\n",
    "function add_noice(start_index, end_index, enc_phonemes_word, probability)\n",
    "    changed_words = Vector{Matrix{Float64}}()\n",
    "\n",
    "    for (i, word_pattern) in enumerate(enc_phonemes_word)\n",
    "        letter_of_word= word_pattern[start_index:end_index,:]\n",
    "        word_pattern[start_index:end_index,:] = flip_spikes_with_probability(letter_of_word, probability)     \n",
    "        push!(changed_words, word_pattern)\n",
    "    end\n",
    "    return changed_words\n",
    "end\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91a6e9",
   "metadata": {},
   "source": [
    "### make spike alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58baca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = [\n",
    "    \n",
    "        \"pɛn\",\"æpəl\",\"bəˈlun\",\"kæt\",\"dɔg\",\"ɛləfənt\",\"flaʊər\",\"gɑrdən\",\"haʊs\",\"maʊntən\",\"tɛn\",\"mɛn\",\"hɛn\",\"dɛn\",\"lɛn\",\"pɛn\",\"jɛn\",\"fɛn\"\n",
    "        # ,\"aɪs\",\"ʤəŋgəl\",\"kaɪt\",\"lɛmən\",\"maʊntən\",\n",
    "        # \"naɪt\",\"oʊʃən\",\"pɛŋgwən\",\"kwin\",\"reɪnˌboʊ\",\"sən\",\"tri\",\"əmˈbrɛlə\",\"vaɪəˈlɪn\",\"wɔtər\",\"zaɪləˌfoʊn\",\"jɑt\",\"zibrə\",\n",
    "        # \"ænt\",\"bərd\",\"klaʊd\",\"dɪˈzərt\",\"dɛzərt\",\"ərθ\",\"fɔrəst\",\"fɔrɪst\",\"greɪp\",\"hɪl\",\"aɪlənd\",\"ʤækət\",\"ʤækɪt\",\n",
    "        # \"ˌkæŋgərˈu\",\"laɪən\",\"məŋki\",\"nɛst\",\"ɔrənʤ\",\"ɔrɪnʤ\",\"pəmkɪn\",\"pəmpkɪn\",\"kwɪlt\",\"rɪvər\",\"sneɪk\",\"taɪgər\",\n",
    "        # \"junɪˌkɔrn\",\"veɪs\",\"vɑz\",\"hweɪl\",\"weɪl\",\"joʊgərt\",  \"dɛn\",\n",
    "# \"dɛn\",\"hɛn\",\"mɛn\",\"tɛn\",\"jɛn\",\"kɛn\",\"lɛn\",\"fɛn\",\"pæn\",\"pɪn\",\"pən\",\"pi\",\"peɪ\",\"pɛp\",\"pər\",\"pɛt\"\n",
    "       \n",
    "  \n",
    "]\n",
    " \n",
    "\n",
    "alphabet = Set{Char}()\n",
    "for word in dictionary\n",
    "    for char in word\n",
    "        push!(alphabet, char)\n",
    "    end\n",
    "end\n",
    "\n",
    "alphabet=collect(alphabet)\n",
    "\n",
    "spike_alphabet = make_spike_alphabet(alphabet, letter_Nsteps, num_Inputs)\n",
    "spike_dict = Dict(alphabet[i] => spike_alphabet[i] for i in 1:length(alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7a79a",
   "metadata": {},
   "source": [
    "### network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82230cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96d1d2b9",
   "metadata": {},
   "source": [
    "### Manually setting input of words - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc77c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_sequences=[\n",
    "\"tɛn\",\"mɛn\",\"hɛn\",\"dɛn\",\"lɛn\",\"pɛn\",\"jɛn\",\"bɛn\"\n",
    "]\n",
    "\n",
    "word_list = [\n",
    "   \"ben\",\"pen\", \"ten\", \"men\", \"hen\", \"den\", \"len\",  \"yen\"\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "print(size(phoneme_sequences))\n",
    "print(size(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caaa9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mapping = Dict(\n",
    "    \"tɛn\" => \"ten\",\n",
    "    \"mɛn\" => \"men\",\n",
    "    \"hɛn\" => \"hen\",\n",
    "    \"dɛn\" => \"den\",\n",
    "    \"lɛn\" => \"len\",\n",
    "    \"pɛn\" => \"pen\",\n",
    "    \"jɛn\" => \"yen\",\n",
    "    \"bɛn\" => \"ben\",\n",
    "    \"gɑrdən\" => \"garden\",\n",
    "    \"haʊs\" => \"house\",\n",
    "    \"maʊntən\" => \"mountain\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_Outputs = length(word_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c133ad58",
   "metadata": {},
   "source": [
    "### Functions to process input and generate labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97198653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_list = [\n",
    "\"pɛn\",\"pɛn\",\"pɛn\",\"pɛn\",\"bɛn\",\"bɛn\",\"bɛn\",\"bɛn\"\n",
    "]\n",
    "max_length_s = maximum(length.(phoneme_sequences))\n",
    "println(max_length_s)\n",
    "max_length_pen = maximum(length.(pen_list))\n",
    "println(max_length_pen)\n",
    "max_length = max(max_length_pen,max_length_s)\n",
    "# max_length =  max_length_s\n",
    "println(max_length)\n",
    "\n",
    "\n",
    "println(size(phoneme_sequences))\n",
    "\n",
    "enc_phonemes = encode_sequences(phoneme_sequences, spike_dict, letter_Nsteps, num_Inputs, max_length)\n",
    "enc_phonemes_pen = encode_sequences(pen_list, spike_dict, letter_Nsteps, num_Inputs, max_length)\n",
    "\n",
    "println(\"type:\", size(enc_phonemes[1]))\n",
    "println(\"type:\", size(enc_phonemes_pen[1]))\n",
    "index_letter_to_change = 1\n",
    "\n",
    "start_index = (index_letter_to_change - 1) * letter_Nsteps + 1\n",
    "end_index = index_letter_to_change * letter_Nsteps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accent_encoded_pen = add_noice(start_index, end_index, enc_phonemes_pen, prob_change)\n",
    "\n",
    "\n",
    "append!(enc_phonemes,accent_encoded_pen)\n",
    "append!(phoneme_sequences,pen_list)\n",
    "\n",
    "trainSize = length(phoneme_sequences)\n",
    "println(trainSize)\n",
    "target=one_hot_encoding(phoneme_sequences,word_list,mapping)\n",
    "println(target)\n",
    "\n",
    "rows, cols = size(enc_phonemes[1])\n",
    "Nsteps = rows\n",
    "x = Array{Int8, 3}(undef, rows, cols, trainSize)\n",
    "for i in 1:trainSize\n",
    "    x[:, :, i]  .= enc_phonemes[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27076314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(size(phoneme_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de08b14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2179b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c7ce457",
   "metadata": {},
   "source": [
    "### Plot Input Spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot first input spike pattern, sanity check\n",
    "function plotInputPattern(x)\n",
    "    dat = copy(x)\n",
    "    dat[dat .< 1]  .= -5\n",
    "    p1 = scatter(dat[:,1], color = \"black\", markersize = 4, alpha = 0.5, ylims = (-1, num_Inputs+1), legend = false, \n",
    "        xlabel = \"Time(ms)\", ylabel = \"Input neuron\")\n",
    "    for j = 2:num_Inputs\n",
    "        scatter!(j*dat[:,j], color = \"black\", markersize = 4, alpha = 0.5)\n",
    "    end\n",
    "    return p1\n",
    "end\n",
    "\n",
    "# for i in 1:trainSize\n",
    "#     display(plotInputPattern(x[:, :, i]))\n",
    "# end\n",
    "# poriginalben=plotInputPattern(x[:,:,2])\n",
    "# poriginal=plotInputPattern(x[:,:,1])\n",
    "\n",
    "# p1 = plotInputPattern(x[:,:,12])\n",
    "# p2 = plotInputPattern(x[:,:,13])\n",
    "# p3 = plotInputPattern(x[:,:,14])\n",
    "# p4 = plotInputPattern(x[:,:,15])\n",
    "\n",
    "# p2 = plotInputPattern(x[:,:,2])\n",
    "# p3= plotInputPattern(x[:,:,3])\n",
    "# display(poriginal)\n",
    "# display(poriginalben)\n",
    "# display(p1)\n",
    "# display(p2)\n",
    "# display(p3)\n",
    "# display(p4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfd7b2",
   "metadata": {},
   "source": [
    "### Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create two sets of weights from input to hidden layer, a, and from hidden to output layer, b\n",
    "    weight_scale = 7*(1.0 - beta)                  # this should give us some spikes to begin with, magic number 7 comes from Zenke\n",
    "    d = Normal(0, weight_scale/sqrt(num_Inputs))   # sample weights from normal distribution with mean 0 and custom variance\n",
    "    \n",
    "    \n",
    "    a = Float32.(rand(d, num_Hidden, num_Inputs))  # input --> hidden weights\n",
    "    b = Float32.(rand(d, num_Outputs, num_Hidden)) # hidden --> output weights\n",
    "    keep_a = copy(a);                              # to check that SG is working, we focus on changes in the \"input\" weights only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52412359",
   "metadata": {},
   "source": [
    "### Spike function and surrogate derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59cba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the spike function\n",
    "function spike(x::Vector{Float64})\n",
    "    (x .> 0)*1            # first creates Boolean which becomes numerial through multiplication; avoid array mutation\n",
    "end\n",
    "\n",
    "# the derivative of the surrogate\n",
    "function SGgrad(x::Vector{Float64})\n",
    "    scale = 10                      # magic scaling factor from Zenke, determines steepness of slope \n",
    "    @. inv((scale*abs(x) + 1.0)^2)  # derivative of the sigmoid below is:  - x /|x|*(|x| + 1)^2 ; for x < 0 this simplifies to 1/(|x| + 1)^2 which is used here; note: derivative of |x| is x/|x|\n",
    "end\n",
    "\n",
    "# set up the custom adjoint; this tells Zygote that the spike function has derivative SGgrad\n",
    "Flux.Zygote.@adjoint function spike(x)\n",
    "    spike(x), Δ -> (Δ.*SGgrad(x), )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12490e6",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time function snn(x::Matrix{Int8}, a::Matrix{Float32}, b::Matrix{Float32}; rec = false)\n",
    "    \n",
    "    if rec\n",
    "        # Record membrane potentials and output spikes\n",
    "        mem_rec = zeros(Nsteps, num_Hidden)\n",
    "        spk_rec = similar(mem_rec)\n",
    "        out_rec = zeros(Nsteps, num_Outputs)\n",
    "    end\n",
    "\n",
    "    # membrane and synaptic time constants\n",
    "    tau_mem = 10e-3\n",
    "    tau_syn = 5e-3\n",
    "    syn_decay = Float32(exp(-dt/tau_syn))\n",
    "    mem_decay = Float32(exp(-dt/tau_mem))\n",
    "\n",
    "    out_rec = zeros(Nsteps, num_Outputs)\n",
    "    # set initial maximum for the two output classes\n",
    "    max_cl = zeros(num_Outputs)\n",
    "        # pre-allocate network arrays, using small floats for speed is probably just silly\n",
    "        syn = Float32.(zeros(num_Hidden))\n",
    "        mem = Float32.(zeros(num_Hidden));\n",
    "        mthr = Float32.(zeros(num_Hidden))\n",
    "        output = Float32.(zeros(num_Outputs))\n",
    "        flt = Float32.(zeros(num_Outputs))\n",
    "\n",
    "        for k = 1:Nsteps\n",
    "\n",
    "            mthr = mem .- 1.0\n",
    "            mthr = spike(mthr)\n",
    "\n",
    "            mem =  (mem_decay*mem + syn)        # update hidden layer membrane potentials\n",
    "            @ignore mem = mem .* (1 .- mthr)    # don't propagate gradients through the spike reset\n",
    "            syn = syn_decay*syn + a*x[k, :]     # update input currents to hidden layer\n",
    "\n",
    "            output = mem_decay*output + flt\n",
    "            flt = syn_decay*flt + b*mthr        # update filtered output\n",
    "\n",
    "            if rec\n",
    "                @ignore mem_rec[k , :] .= mem\n",
    "                @ignore spk_rec[k , :] .= mthr\n",
    "                @ignore out_rec[k , :] .= output\n",
    "            end\n",
    "\n",
    "            max_cl += output\n",
    "\n",
    "  \n",
    "        end\n",
    "        if rec\n",
    "            return max_cl./Nsteps, mem_rec, spk_rec, out_rec\n",
    "        else\n",
    "            return max_cl./Nsteps\n",
    "        end\n",
    "end\n",
    "\n",
    "keep_output, mem_rec, spk_rec, out_rec = snn(x[:,:,2],a,b; rec = true);\n",
    "# println(\"keep_output: \", keep_output)\n",
    "# println(\"mem_rec: \", mem_rec)\n",
    "# println(\"spk_rec: \", spk_rec)\n",
    "# println(\"out_rec: \", out_rec)\n",
    "# maximum(keep_output, dims=1)\n",
    "# plot the previous stuff as a sanity check; now we only run one pattern at a time\n",
    "function plotMembrane(mem::Matrix{Float64}, Nsteps::Int64, spk=nothing)\n",
    "    mem = copy(mem)\n",
    "    if spk != nothing\n",
    "        spike_height = 5\n",
    "        mem[spk .> 0] .= spike_height\n",
    "    end\n",
    "    lower, upper = extrema(mem)\n",
    "    offset = upper/10\n",
    "    plot(mem[1:Nsteps, :], lw = 2, ylim = (lower - offset, upper + offset), legend = false)\n",
    "end\n",
    "p2 = plotMembrane(mem_rec, Nsteps, spk_rec)\n",
    "p3 = plotMembrane(out_rec, Nsteps)\n",
    "p4 = plot(p2, p3, layout = (1,2), xlabel = \"Time (ms)\", ylabel = \"Membrane voltage (a.u.)\", plot_title = \"Hidden layer and readout before training\")\n",
    "display(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebbaa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numEpochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9cc95",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe672e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = Float64[]\n",
    "function trainMe(x::Array{Int8, 3}, a::Array{Float32}, b::Array{Float32}, target::Array{Int8, 2})\n",
    "\n",
    "    optimizer = ADAM(0.0015, (0.9, 0.999))  # first parameter is the learning rate, second parameter tuple controls history dependence of the optimizer\n",
    "\n",
    "        θ = Flux.params(a,b)\n",
    "        loss_hist = []\n",
    "        train_loss = 0\n",
    "\n",
    "        function loss(x::Array{Int8, 2}, a::Array{Float32}, b::Array{Float32}, y::Vector{Int8})\n",
    "            lambda = 0.07                   # scaling factor for regularization\n",
    "            out = snn(x, a, b; rec = false) # model output\n",
    "            println(\"out:\",out)\n",
    "            Flux.logitcrossentropy(out, y) + lambda * sqrt(sum(abs2, a))  # L2-norm regularization\n",
    "        end\n",
    "\n",
    "  \n",
    "    for k=1:numEpochs\n",
    "        \n",
    "        sleep(1)\n",
    "        flush(stdout)\n",
    "        accum_loss = 0\n",
    "\n",
    "        for i in 1:trainSize\n",
    "        # compute gradient of loss evaluated at sample\n",
    "            train_loss, back = pullback(θ) do\n",
    "                # print(loss(x[:,:,i], a, b, target[i,:]))\n",
    "                loss(x[:,:,i], a, b, target[i,:])\n",
    "            end\n",
    "            # println(\"word\",i,\":\",train_loss,\"::\")\n",
    "            accum_loss += train_loss\n",
    "            #println(θ)\n",
    "            update!(optimizer, θ, back(1. ))\n",
    "        end\n",
    "    push!(loss_hist, accum_loss/trainSize)\n",
    "    println(\"Epoch \", k, \" loss: \", accum_loss)\n",
    "    #println(\"Epoch \", k, \" loss: \", loss_hist)\n",
    "    push!(loss_values,accum_loss)\n",
    "    end # epochs\n",
    "    return loss_hist\n",
    "end # end train\n",
    "@time loss_hist = trainMe(x, a, b, target);\n",
    "push!(loss_values_more,loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2675b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(loss_values_more,loss_values)\n",
    "print(size(loss_values_more))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c455d",
   "metadata": {},
   "source": [
    "### loss over epochs most recent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot(1:numEpochs, loss_values, seriestype=:line, xlabel=\"Epochs\", ylabel=\"Loss\", title=\"Loss over Epochs\", grid=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(loss_values)\n",
    "println(size(loss_values))\n",
    "\n",
    "println(size(loss_values_more))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0d334",
   "metadata": {},
   "source": [
    "### loss over epochs per different seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "n = length(loss_values_more)\n",
    "plot_layout = @layout [a b c; d e ]\n",
    "\n",
    "p = plot(layout=plot_layout)\n",
    "\n",
    "for i in 1:n\n",
    "    loss_values = loss_values_more[i]\n",
    "    plot!(p, 1:numEpochs, loss_values, seriestype=:line, xlabel=\"Epochs\", ylabel=\"Loss\", title=\"Loss over Epochs $(i)\", grid=true, subplot=i)\n",
    "end\n",
    "display(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3925e",
   "metadata": {},
   "source": [
    "### average loss (between different probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32134c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numSeries = length(loss_values_more)\n",
    "average_loss = zeros(numEpochs)\n",
    "for i in 1:numEpochs\n",
    "    for j in 1:numSeries\n",
    "        average_loss[i] += loss_values_more[j][i]\n",
    "    end\n",
    "    average_loss[i] /= numSeries\n",
    "end\n",
    "\n",
    "plot(1:numEpochs, average_loss, seriestype=:line, xlabel=\"Epochs\", ylabel=\"Average Loss\", title=\"Average Loss over Epochs\", grid=true,  legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcacedc",
   "metadata": {},
   "source": [
    "### Smoothed average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "numSeries = length(loss_values_more)\n",
    "average_loss = zeros(numEpochs)\n",
    "for i in 1:numEpochs\n",
    "    for j in 1:numSeries\n",
    "        average_loss[i] += loss_values_more[j][i]\n",
    "    end\n",
    "    average_loss[i] /= numSeries\n",
    "end\n",
    "\n",
    "function moving_average(data, window_size)\n",
    "    smoothed_data = zeros(length(data))\n",
    "    for i in 1:length(data)\n",
    "        start_idx = max(1, i - div(window_size, 2))\n",
    "        end_idx = min(length(data), i + div(window_size, 2))\n",
    "        smoothed_data[i] = mean(data[start_idx:end_idx])\n",
    "    end\n",
    "    return smoothed_data\n",
    "end\n",
    "\n",
    "window_size = \n",
    "smoothed_loss = moving_average(average_loss, window_size)\n",
    "plot(1:numEpochs, smoothed_loss, seriestype=:line, xlabel=\"Epochs\", ylabel=\"Loss\", title=\"Smoothed Loss over Epochs\", legend=false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6749f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(diff_smoothed_loss,smoothed_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c0807",
   "metadata": {},
   "source": [
    "### smoothed loss for all probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86867b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(diff_smoothed_loss)\n",
    "print(size(diff_smoothed_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351935ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "n = length(diff_smoothed_loss)\n",
    "plot_layout = @layout [a b c]\n",
    "p = plot(layout=plot_layout)\n",
    "\n",
    "smooth_loss1 = diff_smoothed_loss[1]\n",
    "plot!(p, 1:numEpochs, smooth_loss1, seriestype=:line, xlabel=\"Epochs\", ylabel=\"Loss\", title=\"p=0.60\", grid=true, subplot=1, legend=false)\n",
    "# smooth_loss2 = diff_smoothed_loss[2]\n",
    "# plot!(p, 1:numEpochs, smooth_loss2, seriestype=:line, xlabel=\"Epochs\", ylabel=\"Loss\", title=\"p=0.40\", grid=true, subplot=2, legend=false)\n",
    "# smooth_loss3 = diff_smoothed_loss[3]\n",
    "# plot!(p, 1:numEpochs, smooth_loss1, seriestype=:line, xlabel=\"Epochs\", ylabel=\"Loss\", title=\"p=0.60\", grid=true, subplot=3, legend=false)\n",
    "display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0cfce2",
   "metadata": {},
   "source": [
    "### Preparing test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a98d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_sequences_test = phoneme_sequences\n",
    "\n",
    "max_length_s_test = maximum(length.(phoneme_sequences_test))\n",
    "println(max_length_s_test)\n",
    "# max_length_pen = maximum(length.(pen_list))\n",
    "# println(max_length_pen)\n",
    "# max_length = max(max_length_pen,max_length_s)\n",
    "max_length_test =  max_length_s_test\n",
    "println(max_length_test)\n",
    "\n",
    "  testSize = length(phoneme_sequences_test)\n",
    "  target_test=one_hot_encoding(phoneme_sequences_test,word_list,mapping)\n",
    "  print(target_test)\n",
    "  \n",
    "  enc_phonemes_test = encode_sequences(phoneme_sequences_test, spike_dict, letter_Nsteps, num_Inputs, max_length_test)\n",
    "  \n",
    "  rows_test, cols_test = size(enc_phonemes_test[1])\n",
    "  Nsteps = rows_test\n",
    "  x_test = Array{Int8, 3}(undef, rows_test, cols_test, testSize)\n",
    "  for i in 1:testSize\n",
    "      x_test[:, :, i]  .= enc_phonemes_test[i]\n",
    "  end\n",
    "  \n",
    "\n",
    "\n",
    "println(target_test)\n",
    "println(size(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a4ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8651eaf8",
   "metadata": {},
   "source": [
    "### Re-run model after training and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94621143",
   "metadata": {},
   "outputs": [],
   "source": [
    "function afterTraining(testSize, target_test, word_list)\n",
    "    acc = 0\n",
    "    for j in 1:testSize\n",
    "        keep_output, mem_rec, spk_rec, out_rec = snn(x_test[:,:,j],a,b; rec = true)\n",
    "\n",
    "        println(keep_output)\n",
    "        println(argmax(keep_output),\"-target:\", argmax(target_test[j,:]) )\n",
    "        println(\"choosen score:\",maximum(keep_output))\n",
    "        println(\"target_score:\",keep_output[argmax(target_test[j,:])])\n",
    "        \n",
    "        ((argmax(keep_output)) == argmax(target_test[j,:])) && (acc += 1)\n",
    "        println(acc, word_list[argmax(target_test[j,:])])\n",
    "        println(word_list[argmax(keep_output)])\n",
    "        println(acc, phoneme_sequences_test[j])\n",
    "        \n",
    "\n",
    "        \n",
    "    end\n",
    "return acc/testSize\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "acc = afterTraining(testSize, target_test, word_list)\n",
    "\n",
    "@printf(\"\\nAccuracy: %.4f\\n\", acc)\n",
    "push!(accuracies_normal_words, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3628c0",
   "metadata": {},
   "source": [
    "### prepare for different 'pen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6112b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pen_list_add = [\"bɛn\",\"pɛn\",\"tɛn\",\"mɛn\",\"hɛn\",\"dɛn\",\"lɛn\",\"jɛn\"]\n",
    "\n",
    "enc_phonemes_pen_add = encode_sequences(pen_list_add, spike_dict, letter_Nsteps, num_Inputs, max_length)\n",
    "\n",
    "\n",
    "\n",
    "accent_encoded_pen = add_noice(start_index, end_index, enc_phonemes_pen_add, prob_change)\n",
    "\n",
    "testSize_pen = length(pen_list_add)\n",
    "\n",
    "target_test_pen=one_hot_encoding(pen_list_add,word_list,mapping)\n",
    "\n",
    "\n",
    "rows, cols = size(accent_encoded_pen[1])\n",
    "Nsteps = rows\n",
    "x_test_pen = Array{Int8, 3}(undef, rows, cols, testSize_pen)\n",
    "for i in 1:testSize_pen\n",
    "    x_test_pen[:, :, i]  .= accent_encoded_pen[i]\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8538d98e",
   "metadata": {},
   "source": [
    "### test with different 'pen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function afterTraining(testSize_pen, target_test_pen, word_list)\n",
    "    acc_pen = 0\n",
    "    for j in 1:testSize_pen\n",
    "        keep_output, mem_rec, spk_rec, out_rec = snn(x_test_pen[:,:,j],a,b; rec = true)\n",
    "\n",
    "        println(keep_output)\n",
    "        println(argmax(keep_output),\"-target:\", argmax(target_test_pen[j,:]) )\n",
    "        println(\"choosen score:\",maximum(keep_output))\n",
    "        println(\"target_score:\",keep_output[argmax(target_test_pen[j,:])])\n",
    "        \n",
    "        ((argmax(keep_output)) == argmax(target_test_pen[j,:])) && (acc_pen += 1)\n",
    "        println(acc_pen, word_list[argmax(target_test_pen[j,:])])\n",
    "        println(word_list[argmax(keep_output)])\n",
    "        println(acc_pen, pen_list_add[j])\n",
    "        \n",
    "\n",
    "        \n",
    "    end\n",
    "return acc_pen/testSize_pen\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "acc_pen = afterTraining(testSize_pen, target_test_pen, word_list)\n",
    "\n",
    "@printf(\"\\nAccuracy: %.4f\\n\", acc_pen)\n",
    "push!(accuracies_pen,acc_pen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e96656",
   "metadata": {},
   "source": [
    "### spike pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plot first input spike pattern, sanity check\n",
    "# function plotInputPattern(x)\n",
    "#     dat = copy(x)\n",
    "#     dat[dat .< 1]  .= -5\n",
    "#     p1 = scatter(dat[:,1], color = \"black\", markersize = 4, alpha = 0.5, ylims = (-1, num_Inputs+1), legend = false, \n",
    "#         xlabel = \"Time(ms)\", ylabel = \"Input neuron\")\n",
    "#     for j = 2:num_Inputs\n",
    "#         scatter!(j*dat[:,j], color = \"black\", markersize = 4, alpha = 0.5)\n",
    "#     end\n",
    "#     return p1\n",
    "# end\n",
    "\n",
    "# for i in 1:trainSize\n",
    "#     display(plotInputPattern(x_test_pen[:, :, i]))\n",
    "# end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracies_normal_words)\n",
    "print(accuracies_pen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b3a6e",
   "metadata": {},
   "source": [
    "### calculate average accuracies (between probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "\n",
    "avg = mean(accuracies_normal_words)\n",
    "\n",
    "avg_pen = mean(accuracies_pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg)\n",
    "print(avg_pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "push!(diff_accuracies_probabilities, avg)\n",
    "push!(diff_accuracies_probabilities_pen, avg_pen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54964c22",
   "metadata": {},
   "source": [
    "### barplot accuracy AFter all probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "categories = [\"p=0.20\"\n",
    ", \"p=0.40\", \"p=0.60\"\n",
    "]\n",
    "\n",
    "bar(categories, diff_accuracies_probabilities, legend=false, title=\"Labeling sequences it has trained with\", xlabel=\"Noise Probability\", ylabel=\"Accuracy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfc219",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "categories = [\"p=0.20\"\n",
    ", \"p=0.40\", \"p=0.60\"\n",
    "]\n",
    "\n",
    "bar(categories, diff_accuracies_probabilities_pen, legend=false, title=\"Labeling variations of sequences it has trained with\", xlabel=\"Noise Probability\", ylabel=\"Accuracy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function afterTraining(trainSize, target)\n",
    "#     acc = 0\n",
    "#     for j in 1:trainSize\n",
    "#         keep_output, mem_rec, spk_rec, out_rec = snn(x[:,:,j],a,b; rec = true)\n",
    "\n",
    "#         println(\"argmax output:\",argmax(maximum(keep_output, dims=1)[1,:]))\n",
    "#         println(\"target[j,;]:\",target[j,:])\n",
    "#         println(\"argmax(target[j,:]):\",argmax(target[j,:]))\n",
    "#         println(\"output\",keep_output)\n",
    "#         println(\"maximum(keep_output, dims=1)[1,:]:\",maximum(keep_output, dims=1)[1,:])\n",
    "#         (argmax(maximum(keep_output, dims=1)[1,:]) == argmax(target[j,:])) && (acc += 1)\n",
    "#         println(acc, phoneme_sequences[j])\n",
    "        \n",
    "\n",
    "#         # println(\"keep_output: \", keep_output)\n",
    "#         # println(\"mem_rec: \", mem_rec)\n",
    "#         # println(\"spk_rec: \", spk_rec)\n",
    "#         # println(\"out_rec: \", out_rec)\n",
    "#     end\n",
    "# return acc/trainSize\n",
    "\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6a611",
   "metadata": {},
   "source": [
    "### Plot stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59aaf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_output, mem_rec, spk_rec, out_rec = snn(x[:,:,2],a,b; rec = true)\n",
    "p7 = plotMembrane(mem_rec, Nsteps, spk_rec)\n",
    "p8 = plotMembrane(out_rec, Nsteps)\n",
    "p9 = plot(p7, p8, layout = (1,2), xlabel = \"Time (ms)\", ylabel = \"Membrane voltage (a.u.)\", plot_title = \"Hidden layer and readout after training\")\n",
    "display(p9)\n",
    "\n",
    "p5 = histogram(vcat(keep_a...), bins=:scott, alpha =0.5, label = \"Untrained\")\n",
    "histogram!(vcat(a...), bins=:scott, alpha =0.5, label = \"Trained\", ylabel= \"Frequency\", xlabel = \"Synaptic strength (Input to Hidden)\", palette = :PuOr_4)\n",
    "vline!([mean(vcat(a...))], lw = 3, linestyle = :dash, label = \"Mean trained\")\n",
    "vline!([mean(vcat(keep_a...))], lw = 3, linestyle = :dash, label = \"Mean untrained\")\n",
    "display(p5)\n",
    "\n",
    "p6 = plot(loss_hist, ylabel = \"MSE\", xlabel = \"Epoch\", legend = false)\n",
    "display(p6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
